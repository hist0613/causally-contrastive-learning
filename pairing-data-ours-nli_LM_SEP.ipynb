{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intended-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"CFNLI\"\n",
    "DATASET_SMALLNAME = \"cfnli\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "REFORMED_DATASET_PATH = f\"dataset/{DATASET_NAME}/original_augmented_1x_{DATASET_SMALLNAME}\"\n",
    "OUTPUT_PATH = f\"checkpoints/{DATASET_NAME}/original_augmented_1x_output_scheduling_warmup_lambda_01_0\"\n",
    "REPS_PATH = \"reps\"\n",
    "PICKLE_PATH = f\"dataset/{DATASET_NAME}/cf_augmented_examples\"\n",
    "TRAIN_SPLIT = \"train\"\n",
    "TEST_SPLIT = \"test\"\n",
    "NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honey-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grand-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "#     train = json.load(f) \n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"valid.json\")) as f:\n",
    "#     val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "determined-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = [d['text'] for d in train]\n",
    "# train_labels = [d['label'] for d in train]\n",
    "# val_texts = [d['text'] for d in val]\n",
    "# val_labels = [d['label'] for d in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demographic-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "emerging-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class\n",
    "# train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "# val_dataset = IMDbDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "supposed-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foreign-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "restricted-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, 'best_epoch'), num_labels=NUM_LABELS)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-smooth",
   "metadata": {},
   "source": [
    "## Gradient-based Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "refined-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automatic-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    sp = text.split(\" [SEP] \")\n",
    "    if len(sp) > 1:\n",
    "        return sp\n",
    "    else:\n",
    "        return sp[0]\n",
    "\n",
    "train_texts = [split_sentences(d['anchor_text']) for d in data]\n",
    "train_labels = [d['label'] for d in data]\n",
    "\n",
    "# Encode dataset\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "# make dataset class\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-petite",
   "metadata": {},
   "source": [
    "# Gradient Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "developmental-potter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch['token_type_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.pad_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad[pos_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances\n",
    "\n",
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_token_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch['token_type_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.pad_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['word_embeddings'].weight.grad[token_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['word_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-luxury",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "current-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "def visualize(words, masks):\n",
    "    fig, ax = plt.subplots(figsize=(len(words), 1))\n",
    "    plt.rc('xtick', labelsize=16)\n",
    "    heatmap = sn.heatmap([masks], xticklabels=words, yticklabels=False, square=True, \\\n",
    "                         linewidths=0.1, cmap='coolwarm', center=0.5, vmin=0, vmax=1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def mask_causal_words(tokens, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1][:topk]\n",
    "    for topk_idx in topk_indices:\n",
    "#         print(topk_idx)\n",
    "#         print(tokens[topk_idx])\n",
    "        causal_mask[topk_idx] = 1\n",
    "    \n",
    "    return causal_mask\n",
    "\n",
    "\"\"\"\n",
    "#def mask_causal_words(tokens, importances, topk=1):\n",
    "def mask_causal_words(tokens, importances, topk=-1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    cnt = 0\n",
    "    for topk_idx in topk_indices:\n",
    "#         print(topk_idx)\n",
    "#         print(tokens[topk_idx])\n",
    "        if token[topk_idx] == tokenizer.sep_token_id:\n",
    "            print(\"DEBUG: SEP is detected!!\")\n",
    "            continue\n",
    "        causal_mask[topk_idx] = 1\n",
    "        cnt += 1\n",
    "        if cnt == topk:\n",
    "            break\n",
    "    \n",
    "    return causal_mask\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_importances(data_loader, importance_function):\n",
    "    all_importances = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        importances = importance_function(batch)\n",
    "        all_importances.append(importances)\n",
    "    return all_importances\n",
    "\n",
    "\n",
    "def compute_average_importance(data_loader, all_importances):\n",
    "    all_averaged_importances = []\n",
    "    importance_dict = dict()\n",
    "    importance_dict_counter = dict()\n",
    "    \n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.pad_token_id]]\n",
    "        \n",
    "        for tok_imp, tok in zip(importances, tokens):\n",
    "            assert tok_imp > 0\n",
    "            if not tok in importance_dict.keys():\n",
    "                importance_dict[tok.item()] = 0\n",
    "                importance_dict_counter[tok.item()] = 0\n",
    "            importance_dict[tok.item()] += tok_imp\n",
    "            importance_dict_counter[tok.item()] += 1\n",
    "    \n",
    "    ### [SEP] Token is filtered!!!! ###\n",
    "    importance_dict[tokenizer.sep_token_id] = 0\n",
    "    \n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.pad_token_id]]\n",
    "        averaged_importances = torch.tensor([importance_dict[x.item()]/importance_dict_counter[x.item()] for x in tokens])\n",
    "        all_averaged_importances.append(averaged_importances)\n",
    "    \n",
    "    return all_averaged_importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-municipality",
   "metadata": {},
   "source": [
    "# Compute Gradient-based Causal Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "headed-browse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_causal_mask_with_precomputed(data_loader, all_importances, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        \n",
    "        orig_sample = tokenizer.decode(tokens[:-1])\n",
    "        causal_mask = mask_causal_words(tokens.cpu().numpy(), importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            # print(orig_sample[1], cf_sample[1])\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "                    #noncausal_mask_indices = np.random.choice(noncausal_indices, 1)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "                \n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            ### EDIT FOR NLI: REMOVE SEP TOKEN & SET LABEL ###\n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens[:-1])\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens[:-1])\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'contradiction'\n",
    "            elif labels[0] == 1: label = 'entailment'\n",
    "            else: label = 'neutral'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample, False, 0))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    return triplets, 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-indiana",
   "metadata": {},
   "source": [
    "# Compute Propensity-based Causal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "induced-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _TVD(orig_logits, cf_logits):\n",
    "    return 0.5 * torch.cdist(orig_logits.unsqueeze(0), cf_logits.unsqueeze(0), p=1).squeeze().item()\n",
    "\n",
    "def mask_uniform_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    THRESHOLD = 0.1\n",
    "    uniform_dist = torch.ones(1, 2) / 2.0\n",
    "    uniform_dist = uniform_dist.to(\"cuda\")\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch['token_type_ids'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    best_tvd = 0.\n",
    "    best_idx = -1\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        # For excepting [SEP] token ...\n",
    "        if importances[topk_idx] == 0:\n",
    "            continue\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        orig_tvd = _TVD(torch.softmax(orig_outputs[0], dim=-1), uniform_dist)\n",
    "        masked_tvd = _TVD(torch.softmax(masked_outputs[0], dim=-1), uniform_dist)\n",
    "        \n",
    "        # Use maximum value\n",
    "        if orig_tvd > masked_tvd and abs(orig_tvd - masked_tvd) > best_tvd:\n",
    "            causal_mask[best_idx] = 0\n",
    "            causal_mask[topk_idx] = 1\n",
    "            best_tvd = abs(orig_tvd - masked_tvd)\n",
    "            best_idx = topk_idx\n",
    "        else:\n",
    "            continue\n",
    "        \"\"\"\n",
    "        # Use Gradient Order\n",
    "        if orig_tvd > masked_tvd:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        \"\"\"    \n",
    "        \n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag, best_tvd\n",
    "\n",
    "def mask_softed_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch['token_type_ids'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    best_tvd = 0.\n",
    "    best_idx = -1\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        # For excepting [SEP] token ...\n",
    "        if importances[topk_idx] == 0:\n",
    "            continue\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        tvd_value = _TVD(torch.softmax(orig_outputs[0], dim=-1), torch.softmax(masked_outputs[0], dim=-1))\n",
    "        \n",
    "        # Use Maximum  Value\n",
    "        if tvd_value > best_tvd:\n",
    "            causal_mask[best_idx] = 0\n",
    "            causal_mask[topk_idx] = 1\n",
    "            best_tvd = tvd_value\n",
    "            best_idx = topk_idx\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        \"\"\"\n",
    "        # Use Gradeint Order\n",
    "        if orig_tvd > MIN_FLIPPED:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        \"\"\"\n",
    "        \n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag, best_tvd\n",
    "\n",
    "def mask_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch['token_type_ids'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        # For excepting [SEP] token ...\n",
    "        if importances[topk_idx] == 0:\n",
    "            continue\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        if orig_prediction != masked_prediction:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag, 0\n",
    "\n",
    "def build_propensity_causal_mask_with_precomputed(data_loader, all_importances, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    no_flip_cnt = 0\n",
    "    no_flip_idx = []\n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        if len(tokens) == 0:\n",
    "            print(batch['input_ids'][0])\n",
    "            triplets.append((label, orig_sample, orig_sample, orig_sample, True, 0))\n",
    "            continue\n",
    "        orig_sample = tokenizer.decode(tokens[:-1])\n",
    "        #causal_mask, err_flag, maximum_score = mask_propensity_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        #causal_mask, err_flag, maximum_score = mask_softed_propensity_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        causal_mask, err_flag, maximum_score = mask_uniform_propensity_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        no_flip_idx.append(err_flag)\n",
    "        if err_flag:\n",
    "            no_flip_cnt += 1\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            print(tokens)\n",
    "            triplets.append((label, orig_sample, orig_sample, orig_sample, err_flag, maximum_score))\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "                    #noncausal_mask_indices = np.random.choice(noncausal_indices, 1)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            ### EDIT FOR NLI: REMOVE SEP TOKEN & SET LABEL ###\n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens[:-1])\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens[:-1])\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'contradiction'\n",
    "            elif labels[0] == 1: label = 'entailment'\n",
    "            else: label = 'neutral'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample, err_flag, maximum_score))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    print(f\"No Flip Cnt: {no_flip_cnt}\")    \n",
    "    return triplets, no_flip_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-criminal",
   "metadata": {},
   "source": [
    "# Compute LM-based Causal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interesting-broadcast",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "mlm_model = mlm_model.to(device)\n",
    "mlm_model.eval()\n",
    "TOPK_NUM = 4\n",
    "\n",
    "def mask_efficient_LM_dropout_causal_words(tokens, batch, importances, topk=1):\n",
    "    dropout = torch.nn.Dropout(0.5)\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    find_flag = False\n",
    "    \n",
    "    #Skip neutral sample\n",
    "    if torch.max(batch['labels'].squeeze()).item() == 2:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "        return causal_mask, err_flag, 0\n",
    "    \n",
    "    input_ids = batch['input_ids'].squeeze().repeat((TOPK_NUM, )).reshape(TOPK_NUM, -1).to(device)\n",
    "    attention_mask = batch['attention_mask'].expand(TOPK_NUM, -1).to(device)\n",
    "    token_type_ids = batch['token_type_ids'].expand(TOPK_NUM, -1).to(device)\n",
    "    \n",
    "    masked_input_ids = batch['input_ids'].squeeze().repeat((len(tokens), )).reshape(len(tokens), -1).to(device)\n",
    "    masked_attention_mask = batch['attention_mask'].squeeze().repeat((len(tokens), )).reshape(len(tokens), -1).to(device)\n",
    "    masked_token_type_ids = batch['token_type_ids'].expand(len(tokens), -1).to(device)\n",
    "    fake_labels = torch.ones((len(tokens),))\n",
    "    masked_train = IMDbDataset({\n",
    "        'input_ids': masked_input_ids, \n",
    "        'attention_mask': masked_attention_mask, \n",
    "        'token_type_ids': masked_token_type_ids, \n",
    "        'topk_indices': topk_indices}, fake_labels)\n",
    "    masked_train_loader = DataLoader(masked_train, batch_size=4, shuffle=False)\n",
    "    logits = []\n",
    "    for masked_batch in masked_train_loader:\n",
    "        masked_input_ids = masked_batch['input_ids'].to(device)\n",
    "        masked_attention_mask = masked_batch['attention_mask'].to(device)\n",
    "        masked_token_type_ids = masked_batch['token_type_ids'].to(device)\n",
    "        topk_index = masked_batch['topk_indices'].to(device)\n",
    "        masked_input_embeds = mlm_model.bert.embeddings.word_embeddings(masked_input_ids)\n",
    "        for mi_idx, topk_idx in zip(range(masked_input_embeds.size(0)), topk_index):\n",
    "            masked_input_embeds[mi_idx][topk_idx + 1] = dropout(masked_input_embeds[mi_idx][topk_idx + 1])\n",
    "            if masked_token_type_ids[mi_idx][topk_idx + 1] == 0:\n",
    "                masked_attention_mask[mi_idx] = ~masked_token_type_ids[mi_idx] & masked_attention_mask[mi_idx]\n",
    "            else:\n",
    "                masked_attention_mask[mi_idx] = masked_token_type_ids[mi_idx] & masked_attention_mask[mi_idx]\n",
    "                masked_attention_mask[mi_idx][0] = 1\n",
    "        with torch.no_grad():\n",
    "            outputs = mlm_model(attention_mask=masked_attention_mask, inputs_embeds=masked_input_embeds)\n",
    "            predictions = outputs[0]\n",
    "            #logits.append(predictions.detach().cpu())\n",
    "        \n",
    "        topk_logits = torch.topk(predictions, TOPK_NUM, dim=-1)[1]\n",
    "        mask_candidates = [topk_logit[topk_idx + 1] for topk_idx, topk_logit in zip(topk_index, topk_logits)]\n",
    "        \n",
    "        for topk_idx, mask_candidate in zip(topk_index, mask_candidates):\n",
    "            # For excepting [SEP] token ...\n",
    "            if importances[topk_idx] == 0:\n",
    "                continue\n",
    "            recon_input_ids = input_ids.clone()\n",
    "            for i, mc in enumerate(mask_candidate):\n",
    "                recon_input_ids[i][topk_idx + 1] = mc\n",
    "             \n",
    "            with torch.no_grad():\n",
    "                recon_outputs = model(recon_input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                _, recon_prediction = torch.max(recon_outputs[0], dim=1)\n",
    "                        \n",
    "            # IF prediction is changed:\n",
    "            if len(torch.unique(recon_prediction)) != 1:\n",
    "                \"\"\"\n",
    "                print(tokenizer.decode(recon_input_ids[0], skip_special_tokens=True))\n",
    "                print(tokenizer.decode(recon_input_ids[1], skip_special_tokens=True))\n",
    "                print(tokenizer.decode(recon_input_ids[2], skip_special_tokens=True))\n",
    "                print(tokenizer.decode(recon_input_ids[3], skip_special_tokens=True))\n",
    "                print(recon_prediction)\n",
    "                \"\"\"\n",
    "                causal_mask[topk_idx] = 1\n",
    "                find_flag = True\n",
    "                break\n",
    "            \n",
    "            \"\"\"\n",
    "            # IF prediction has entail and contradiction\n",
    "            if 0 in recon_prediction and 1 in recon_prediction:\n",
    "                causal_mask[topk_idx] = 1\n",
    "                find_flag = True\n",
    "                break\n",
    "            \"\"\"    \n",
    "        if find_flag:\n",
    "            break\n",
    "    \n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    \n",
    "    return causal_mask, err_flag, 0\n",
    "\n",
    "\"\"\"\n",
    "def mask_LM_dropout_causal_words(tokens, batch, importances, topk=1):\n",
    "    dropout = torch.nn.Dropout(0.5)\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    \n",
    "    masked_input_ids = batch['input_ids'].squeeze().repeat((len(tokens), )).reshape(len(tokens), -1).to(device)\n",
    "    masked_attention_mask = batch['attention_mask'].expand(len(tokens), -1).to(device)\n",
    "    masked_token_type_ids = batch['token_type_ids'].expand(len(tokens), -1).to(device)\n",
    "    fake_labels = torch.ones((len(tokens),))\n",
    "    masked_train = IMDbDataset({'input_ids': masked_input_ids, 'attention_mask': masked_attention_mask, 'token_type_ids': masked_token_type_ids, 'topk_indices': topk_indices}, fake_labels)\n",
    "    masked_train_loader = DataLoader(masked_train, batch_size=8, shuffle=False)\n",
    "    logits = []\n",
    "    for masked_batch in masked_train_loader:\n",
    "        masked_input_ids = masked_batch['input_ids'].to(device)\n",
    "        masked_attention_mask = masked_batch['attention_mask'].to(device)\n",
    "        masked_token_type_ids = masked_batch['token_type_ids'].to(device)\n",
    "        topk_index = masked_batch['topk_indices'].to(device)\n",
    "        masked_input_embeds = mlm_model.bert.embeddings.word_embeddings(masked_input_ids)\n",
    "        for mie, ti in zip(masked_input_embeds, topk_index):\n",
    "            mie[ti + 1] = dropout(mie[ti + 1])\n",
    "        with torch.no_grad():\n",
    "            outputs = mlm_model(attention_mask=masked_attention_mask, token_type_ids=masked_token_type_ids, inputs_embeds=masked_input_embeds)\n",
    "            predictions = outputs[0]\n",
    "            logits.append(predictions)\n",
    "            \n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    topk_logits = torch.topk(logits, TOPK_NUM, dim=-1)[1]\n",
    "    mask_candidates = [topk_logit[topk_idx + 1] for topk_idx, topk_logit in zip(topk_indices, topk_logits)]\n",
    "    \n",
    "    input_ids = batch['input_ids'].squeeze().repeat((TOPK_NUM, )).reshape(TOPK_NUM, -1).to(device)\n",
    "    attention_mask = batch['attention_mask'].expand(TOPK_NUM, -1).to(device)\n",
    "    token_type_ids = batch['token_type_ids'].expand(TOPK_NUM, -1).to(device)\n",
    "    \n",
    "    for topk_idx, mask_candidate in zip(topk_indices, mask_candidates):\n",
    "        recon_input_ids = input_ids.clone()\n",
    "        for i, mc in enumerate(mask_candidate):\n",
    "            recon_input_ids[i][topk_idx + 1] = mc\n",
    "             \n",
    "        with torch.no_grad():\n",
    "            recon_outputs = model(recon_input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            _, recon_prediction = torch.max(recon_outputs[0], dim=1)\n",
    "        \n",
    "        # IF prediction is changed:\n",
    "        if len(torch.unique(recon_prediction)) != 1:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "    \n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    \n",
    "    return causal_mask, err_flag, 0\n",
    "\n",
    "def mask_LM_causal_words(tokens, batch, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "\n",
    "    masked_input_ids = batch['input_ids'].squeeze().repeat((len(tokens), )).reshape(len(tokens), -1).to(device)\n",
    "    masked_attention_mask = batch['attention_mask'].expand(len(tokens), -1).to(device)\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        masked_input_ids[i][topk_idx + 1] = tokenizer.mask_token_id\n",
    "    fake_labels = torch.ones((len(tokens),))\n",
    "    masked_train = IMDbDataset({'input_ids': masked_input_ids, 'attention_mask': masked_attention_mask}, fake_labels)\n",
    "    masked_train_loader = DataLoader(masked_train, batch_size=8, shuffle=False)\n",
    "    logits = []\n",
    "    for masked_batch in masked_train_loader:\n",
    "        masked_input_ids = masked_batch['input_ids'].to(device)\n",
    "        masked_attention_mask = masked_batch['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = mlm_model(masked_input_ids, attention_mask=masked_attention_mask)\n",
    "            predictions = outputs[0]\n",
    "            logits.append(predictions)\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    topk_logits = torch.topk(logits, TOPK_NUM, dim=-1)[1]\n",
    "    mask_candidates = [topk_logit[topk_idx + 1] for topk_idx, topk_logit in zip(topk_indices, topk_logits)]\n",
    "    \n",
    "    input_ids = batch['input_ids'].squeeze().repeat((TOPK_NUM, )).reshape(TOPK_NUM, -1).to(device)\n",
    "    attention_mask = batch['attention_mask'].expand(TOPK_NUM, -1).to(device)\n",
    "    for topk_idx, mask_candidate in zip(topk_indices, mask_candidates):\n",
    "        recon_input_ids = input_ids.clone()\n",
    "        for i, mc in enumerate(mask_candidate):\n",
    "            recon_input_ids[i][topk_idx + 1] = mc\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            recon_outputs = model(recon_input_ids, attention_mask=attention_mask)\n",
    "            _, recon_prediction = torch.max(recon_outputs[0], dim=1)\n",
    "        \n",
    "        # IF prediction is changed:\n",
    "        if len(torch.unique(recon_prediction)) != 1:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "    \n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    \n",
    "    return causal_mask, err_flag, 0\n",
    "\"\"\"\n",
    "\n",
    "def build_LM_causal_mask_with_precomputed(data_loader, all_importances, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    no_flip_cnt = 0\n",
    "    no_flip_idx = []\n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "\n",
    "        orig_sample = tokenizer.decode(tokens[:-1])\n",
    "        causal_mask, err_flag, maximum_score = mask_efficient_LM_dropout_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        no_flip_idx.append(err_flag)\n",
    "        if err_flag:\n",
    "            no_flip_cnt += 1\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            print(tokens)\n",
    "            triplets.append((label, orig_sample, orig_sample, orig_sample, err_flag, maximum_score))\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "                    #noncausal_mask_indices = np.random.choice(noncausal_indices, 1)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            ### EDIT FOR NLI: REMOVE SEP TOKEN & SET LABEL ###\n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens[:-1])\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens[:-1])\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'contradiction'\n",
    "            elif labels[0] == 1: label = 'entailment'\n",
    "            else: label = 'neutral'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample, err_flag, maximum_score))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    print(f\"No Flip Cnt: {no_flip_cnt}\")    \n",
    "    return triplets, no_flip_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-characteristic",
   "metadata": {},
   "source": [
    "# Compute OR Load Gradient Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "expired-lecture",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PICKLE_PATH):\n",
    "    os.makedirs(PICKLE_PATH)\n",
    "\n",
    "#all_importance = compute_importances(train_loader, get_gradient_norms)\n",
    "#with open(\"dataset/SST-2/cf_augmented_examples/gradient_importance.pickle\", 'wb') as f:\n",
    "#    pickle.dump(all_importance, f)\n",
    "\n",
    "if os.path.exists(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\")):\n",
    "    with open(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\"), 'rb') as f:\n",
    "        all_importance = pickle.load(f)\n",
    "else:\n",
    "    all_importance = compute_importances(train_loader, get_gradient_norms)\n",
    "    with open(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\"), 'wb') as f:\n",
    "        pickle.dump(all_importance, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-beatles",
   "metadata": {},
   "source": [
    "# Get Average Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cognitive-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212f81a3cb8443d19bf0bb698121be49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933c3075291e4158948ccec1be3be01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "averaged_all_importance = compute_average_importance(train_loader, all_importance)\n",
    "with open(os.path.join(PICKLE_PATH, \"gradient_averaged_importance.pickle\"), 'wb') as f:\n",
    "        pickle.dump(averaged_all_importance, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-chase",
   "metadata": {},
   "source": [
    "# Generate Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "continued-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e32bdbdfb5492da3a5ef5c0af6368d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Cnt: 0\n",
      "No Flip Cnt: 746\n"
     ]
    }
   ],
   "source": [
    "sampling_ratio = 1\n",
    "augment_ratio = 1\n",
    "#triplets_train, no_flip_idx_train = build_propensity_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)\n",
    "triplets_train, no_flip_idx_train = build_LM_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "minor-minutes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('entailment',\n",
       " 'a woman in a red polka - dot dress sings into a mic. [SEP] a woman is wearing a dress.',\n",
       " 'a [MASK] in a red polka - dot dress sings into a mic. [SEP] a woman is wearing a dress.',\n",
       " 'a woman in a red polka - dot dress sings into a mic. [SEP] a [MASK] is wearing a dress.',\n",
       " False,\n",
       " 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strategic-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_LM_dropout_05_flip_sep_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "    pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-hygiene",
   "metadata": {},
   "source": [
    "# Cut with Empirically  Setted Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.0001\n",
    "threshold_cnt = 0\n",
    "threshold_triplets_train = []\n",
    "for t in triplets_train:\n",
    "    t = list(t)\n",
    "    if t[-1] < THRESHOLD:\n",
    "        t[-2] = True\n",
    "        threshold_cnt += 1\n",
    "    t = tuple(t)\n",
    "    threshold_triplets_train.append(t)\n",
    "print(threshold_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_TVD_uniform_thres_00001_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "    pickle.dump(threshold_triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "###For Generating Optionally\n",
    "\n",
    "#for sampling_ratio in [1, 2, 3, 4, 5]:\n",
    "for sampling_ratio in [1]:\n",
    "    for augment_ratio in [1]:\n",
    "        triplets_train, no_flip_idx_train = build_propensity_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)\n",
    "        with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "            pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FLIPPED = 0.007849693298339844 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sorted(unflipped_TVD_logit)\n",
    "print(min(flipped_TVD_logit))\n",
    "print(c[int(len(c) * 0.90)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-prescription",
   "metadata": {},
   "source": [
    "# Qualitative Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHELL FOR QUALITATIVE: WILL BE REMOVED\\\n",
    "sampling_ratio = 1\n",
    "augment_ratio = 1\n",
    "triplets_train, no_flip_idx_train = build_propensity_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=None, augment_ratio=augment_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_triplets = [tt for tt in triplets_train if tt[2].count(\"[MASK]\") > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_triplets[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tt in enumerate(triplets_train):\n",
    "    if tt[1] == 'commercialism all in the same movie... without neglecting character development for even one minute':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_sampling1_augmenting1_train.pickle\".format(sampling_ratio, augment_ratio)), \"rb\") as fp:\n",
    "            data_b = pickle.load(fp)\n",
    "        \n",
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_sampling1_augmenting1_train.pickle\".format(sampling_ratio, augment_ratio)), \"rb\") as fp:\n",
    "            data_a = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "diff_idxes = []\n",
    "for i, (a, b) in enumerate(zip(data_a, data_b)):\n",
    "    if a[2] != b[2]:\n",
    "        diff_idxes.append(i)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_i = 3\n",
    "print(\"[ORIG]:    \" + data_a[diff_idxes[tmp_i]][1])\n",
    "print()\n",
    "print(\"[GRAD]:    \" + data_a[diff_idxes[tmp_i]][2])\n",
    "print()\n",
    "print(\"[PROP]:    \" + data_b[diff_idxes[tmp_i]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4,sci_mode=False)\n",
    "print(tokenizer.tokenize('commercialism all in the same movie... without neglecting character development for even one minute'))\n",
    "print(averaged_all_importance[1590])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a[1590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "9719 / - (53349 - 67349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
