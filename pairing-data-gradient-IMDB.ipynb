{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intended-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marked-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "REFORMED_DATASET_PATH = \"dataset/SST-2/original_augmented_1x_sst2\"\n",
    "OUTPUT_PATH = \"checkpoints/SST-2/original_augmented_1x_output_scheduling_warmup_lambda_01_2\"\n",
    "REPS_PATH = \"reps\"\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "TRAIN_SPLIT = \"train\"\n",
    "TEST_SPLIT = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honey-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grand-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "#     train = json.load(f) \n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"valid.json\")) as f:\n",
    "#     val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "determined-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = [d['text'] for d in train]\n",
    "# train_labels = [d['label'] for d in train]\n",
    "# val_texts = [d['text'] for d in val]\n",
    "# val_labels = [d['label'] for d in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demographic-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "emerging-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class\n",
    "# train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "# val_dataset = IMDbDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supposed-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "foreign-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "restricted-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, 'epoch_2'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-smooth",
   "metadata": {},
   "source": [
    "## Gradient-based Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "refined-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automatic-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [d['anchor_text'] for d in data]\n",
    "train_labels = [d['label'] for d in data]\n",
    "\n",
    "# Encode dataset\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "# make dataset class\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "developmental-potter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad[pos_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-browse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d0aa6c4a5d4ac2ac01402043d6e027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=67349), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Cnt: 1978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8ec2f259a241d5b2702e8cb8a7a636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=67349), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "def visualize(words, masks):\n",
    "    fig, ax = plt.subplots(figsize=(len(words), 1))\n",
    "    plt.rc('xtick', labelsize=16)\n",
    "    heatmap = sn.heatmap([masks], xticklabels=words, yticklabels=False, square=True, \\\n",
    "                         linewidths=0.1, cmap='coolwarm', center=0.5, vmin=0, vmax=1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "def mask_causal_words(tokens, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1][:topk]\n",
    "    for topk_idx in topk_indices:\n",
    "#         print(topk_idx)\n",
    "#         print(tokens[topk_idx])\n",
    "        causal_mask[topk_idx] = 1\n",
    "    \n",
    "    return causal_mask\n",
    "\n",
    "def build_causal_mask(data_loader, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        importances = get_gradient_norms(batch)\n",
    "        \n",
    "        \n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        \n",
    "        orig_sample = tokenizer.decode(tokens)\n",
    "        causal_mask = mask_causal_words(tokens.cpu().numpy(), importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            # print(orig_sample[1], cf_sample[1])\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, sampling_ratio)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens)\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens)\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'Negative'\n",
    "            elif labels[0] == 1: label = 'Positive'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    return triplets\n",
    "\n",
    "import pickle\n",
    "\n",
    "# sampling_ratio = 2\n",
    "# augment_ratio = 1\n",
    "os.mkdir(\"dataset/SST-2/cf_augmented_examples\")\n",
    "for sampling_ratio in [1, 2, 3]:\n",
    "    for augment_ratio in [1]:\n",
    "        triplets_train = build_causal_mask(train_loader, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)\n",
    "        with open(\"dataset/SST-2/cf_augmented_examples/triplets_automated_gradient_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio), \"wb\") as fp:\n",
    "            pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "requested-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"dataset/FineFood/cf_augmented_examples\")\n",
    "with open(\"dataset/FineFood/cf_augmented_examples/triplets_automated_gradient_sampling1_augmenting1_train.pickle\", 'wb') as fp:\n",
    "    pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-testing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
