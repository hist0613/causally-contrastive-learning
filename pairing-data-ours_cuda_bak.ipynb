{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intended-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marked-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"IMDb\"\n",
    "DATASET_SMALLNAME = \"aclImdb\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "REFORMED_DATASET_PATH = f\"dataset/{DATASET_NAME}/original_augmented_1x_{DATASET_SMALLNAME}\"\n",
    "OUTPUT_PATH = f\"checkpoints/{DATASET_NAME}/original_augmented_1x_output_scheduling_warmup_bs8_2\"\n",
    "REPS_PATH = \"reps\"\n",
    "PICKLE_PATH = f\"dataset/{DATASET_NAME}/cf_augmented_examples\"\n",
    "TRAIN_SPLIT = \"train\"\n",
    "TEST_SPLIT = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "honey-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grand-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "#     train = json.load(f) \n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"valid.json\")) as f:\n",
    "#     val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "determined-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = [d['text'] for d in train]\n",
    "# train_labels = [d['label'] for d in train]\n",
    "# val_texts = [d['text'] for d in val]\n",
    "# val_labels = [d['label'] for d in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demographic-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "emerging-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class\n",
    "# train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "# val_dataset = IMDbDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supposed-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "foreign-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "restricted-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, 'epoch_2'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-smooth",
   "metadata": {},
   "source": [
    "## Gradient-based Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "refined-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automatic-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [d['anchor_text'] for d in data]\n",
    "train_labels = [d['label'] for d in data]\n",
    "\n",
    "# Encode dataset\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "# make dataset class\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "developmental-potter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad[pos_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "affected-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_token_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['word_embeddings'].weight.grad[token_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['word_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "headed-browse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "def visualize(words, masks):\n",
    "    fig, ax = plt.subplots(figsize=(len(words), 1))\n",
    "    plt.rc('xtick', labelsize=16)\n",
    "    heatmap = sn.heatmap([masks], xticklabels=words, yticklabels=False, square=True, \\\n",
    "                         linewidths=0.1, cmap='coolwarm', center=0.5, vmin=0, vmax=1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def mask_causal_words(tokens, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1][:topk]\n",
    "    for topk_idx in topk_indices:\n",
    "#         print(topk_idx)\n",
    "#         print(tokens[topk_idx])\n",
    "        causal_mask[topk_idx] = 1\n",
    "    \n",
    "    return causal_mask\n",
    "\n",
    "def compute_importances(data_loader, importance_function):\n",
    "    all_importances = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        importances = importance_function(batch)\n",
    "        all_importances.append(importances)\n",
    "    return all_importances\n",
    "\n",
    "\n",
    "\n",
    "def build_causal_mask_with_precomputed(data_loader, all_importances, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        \n",
    "        orig_sample = tokenizer.decode(tokens)\n",
    "        causal_mask = mask_causal_words(tokens.cpu().numpy(), importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            # print(orig_sample[1], cf_sample[1])\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "                    #noncausal_mask_indices = np.random.choice(noncausal_indices, 1)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens)\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens)\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'Negative'\n",
    "            elif labels[0] == 1: label = 'Positive'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    return triplets\n",
    "\n",
    "def mask_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        if orig_prediction != masked_prediction:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag \n",
    "\n",
    "def build_propensity_causal_mask_with_precomputed(data_loader, all_importances, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    no_flip_cnt = 0\n",
    "    no_flip_idx = []\n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        \n",
    "        orig_sample = tokenizer.decode(tokens)\n",
    "        causal_mask, err_flag = mask_propensity_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        no_flip_idx.append(err_flag)\n",
    "        if err_flag:\n",
    "            no_flip_cnt += 1\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            # print(orig_sample[1], cf_sample[1])\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "                    #noncausal_mask_indices = np.random.choice(noncausal_indices, 1)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens)\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens)\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'Negative'\n",
    "            elif labels[0] == 1: label = 'Positive'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample, err_flag))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    print(f\"No Flip Cnt: {no_flip_cnt}\")    \n",
    "    return triplets, no_flip_idx\n",
    "\n",
    "def compute_average_importance(data_loader, all_importances):\n",
    "    all_averaged_importances = []\n",
    "    importance_dict = dict()\n",
    "    importance_dict_counter = dict()\n",
    "    \n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]]\n",
    "        \n",
    "        for tok_imp, tok in zip(importances, tokens):\n",
    "            if not tok in importance_dict.keys():\n",
    "                importance_dict[tok.item()] = 0\n",
    "                importance_dict_counter[tok.item()] = 0\n",
    "            importance_dict[tok.item()] += tok_imp\n",
    "            importance_dict_counter[tok.item()] += 1\n",
    "            \n",
    "    \n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]]\n",
    "        averaged_importances = torch.tensor([importance_dict[x.item()]/importance_dict_counter[x.item()] for x in tokens])\n",
    "        all_averaged_importances.append(averaged_importances)\n",
    "    \n",
    "    return all_averaged_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "expired-lecture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PICKLE_PATH):\n",
    "    os.makedirs(PICKLE_PATH)\n",
    "\n",
    "#all_importance = compute_importances(train_loader, get_gradient_norms)\n",
    "#with open(\"dataset/SST-2/cf_augmented_examples/gradient_importance.pickle\", 'wb') as f:\n",
    "#    pickle.dump(all_importance, f)\n",
    "\n",
    "if os.path.exists(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\")):\n",
    "    with open(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\"), 'rb') as f:\n",
    "        all_importance = pickle.load(f)\n",
    "else:\n",
    "    all_importance = compute_importances(train_loader, get_gradient_norms)\n",
    "    with open(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\"), 'wb') as f:\n",
    "        pickle.dump(all_importance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cognitive-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb1b18ddd454faca897462c3246c03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bbc70c68814932aef7ff98698258b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "averaged_all_importance = compute_average_importance(train_loader, all_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "coordinate-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"gradient_averaged_importance.pickle\"), 'wb') as f:\n",
    "        pickle.dump(averaged_all_importance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-chicago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced2c4c9ed8b44ecb8194180bfc8e3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for sampling_ratio in [1, 2, 3, 4, 5]:\n",
    "for sampling_ratio in [1]:\n",
    "    for augment_ratio in [1]:\n",
    "        triplets_train, no_flip_idx_train = build_propensity_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)\n",
    "        with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "            pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "continued-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_sampling{}_augmenting{}_flip_idx_train.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "    pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dimensional-aviation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Negative',\n",
       " 'hide new secretions from the parental units',\n",
       " 'hide new secretions from the [MASK] units',\n",
       " 'hide new secret [MASK] from the parental units',\n",
       " True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-prescription",
   "metadata": {},
   "source": [
    "# Qualitative Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alien-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_sampling1_augmenting1_train.pickle\".format(sampling_ratio, augment_ratio)), \"rb\") as fp:\n",
    "            data_b = pickle.load(fp)\n",
    "        \n",
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_sampling1_augmenting1_train.pickle\".format(sampling_ratio, augment_ratio)), \"rb\") as fp:\n",
    "            data_a = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "frequent-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "diff_idxes = []\n",
    "for i, (a, b) in enumerate(zip(data_a, data_b)):\n",
    "    if a[2] != b[2]:\n",
    "        diff_idxes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "labeled-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORIG]:    goes to absurd lengths\n",
      "\n",
      "[GRAD]:    goes to absurd [MASK]\n",
      "\n",
      "[PROP]:    goes to [MASK] lengths\n"
     ]
    }
   ],
   "source": [
    "tmp_i = 3\n",
    "print(\"[ORIG]:    \" + data_a[diff_idxes[tmp_i]][1])\n",
    "print()\n",
    "print(\"[GRAD]:    \" + data_a[diff_idxes[tmp_i]][2])\n",
    "print()\n",
    "print(\"[PROP]:    \" + data_b[diff_idxes[tmp_i]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-figure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
