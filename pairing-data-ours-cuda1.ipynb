{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intended-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marked-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_SAMPLES = 8000\n",
    "DATASET_NAME = \"IMDb\"\n",
    "DATASET_SMALLNAME = \"aclImdb\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "REFORMED_DATASET_PATH = f\"dataset/{DATASET_NAME}/original_augmented_1x_{DATASET_SMALLNAME}\"\n",
    "OUTPUT_PATH = f\"checkpoints/{DATASET_NAME}/original_augmented_1x_output_scheduling_warmup_bs8_2\"\n",
    "REPS_PATH = \"reps\"\n",
    "PICKLE_PATH = f\"dataset/{DATASET_NAME}/cf_augmented_examples\"\n",
    "TRAIN_SPLIT = \"train\"\n",
    "TEST_SPLIT = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honey-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grand-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "#     train = json.load(f) \n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"valid.json\")) as f:\n",
    "#     val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "determined-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = [d['text'] for d in train]\n",
    "# train_labels = [d['label'] for d in train]\n",
    "# val_texts = [d['text'] for d in val]\n",
    "# val_labels = [d['label'] for d in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demographic-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "emerging-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class\n",
    "# train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "# val_dataset = IMDbDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "supposed-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foreign-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "restricted-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, 'epoch_2'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-smooth",
   "metadata": {},
   "source": [
    "## Gradient-based Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "refined-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automatic-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [d['anchor_text'] for d in data][SPLIT_SAMPLES:SPLIT_SAMPLES*2]\n",
    "train_labels = [d['label'] for d in data][SPLIT_SAMPLES:SPLIT_SAMPLES*2]\n",
    "\n",
    "# Encode dataset\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "# make dataset class\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-petite",
   "metadata": {},
   "source": [
    "# Gradient Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "developmental-potter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad[pos_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances\n",
    "\n",
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_token_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['word_embeddings'].weight.grad[token_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['word_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-luxury",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "current-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "def visualize(words, masks):\n",
    "    fig, ax = plt.subplots(figsize=(len(words), 1))\n",
    "    plt.rc('xtick', labelsize=16)\n",
    "    heatmap = sn.heatmap([masks], xticklabels=words, yticklabels=False, square=True, \\\n",
    "                         linewidths=0.1, cmap='coolwarm', center=0.5, vmin=0, vmax=1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def mask_causal_words(tokens, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1][:topk]\n",
    "    for topk_idx in topk_indices:\n",
    "#         print(topk_idx)\n",
    "#         print(tokens[topk_idx])\n",
    "        causal_mask[topk_idx] = 1\n",
    "    \n",
    "    return causal_mask\n",
    "\n",
    "def compute_importances(data_loader, importance_function):\n",
    "    all_importances = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        importances = importance_function(batch)\n",
    "        all_importances.append(importances)\n",
    "    return all_importances\n",
    "\n",
    "\n",
    "def compute_average_importance(data_loader, all_importances):\n",
    "    all_averaged_importances = []\n",
    "    importance_dict = dict()\n",
    "    importance_dict_counter = dict()\n",
    "    \n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]]\n",
    "        \n",
    "        for tok_imp, tok in zip(importances, tokens):\n",
    "            if not tok in importance_dict.keys():\n",
    "                importance_dict[tok.item()] = 0\n",
    "                importance_dict_counter[tok.item()] = 0\n",
    "            importance_dict[tok.item()] += tok_imp\n",
    "            importance_dict_counter[tok.item()] += 1\n",
    "            \n",
    "    \n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]]\n",
    "        averaged_importances = torch.tensor([importance_dict[x.item()]/importance_dict_counter[x.item()] for x in tokens])\n",
    "        all_averaged_importances.append(averaged_importances)\n",
    "    \n",
    "    return all_averaged_importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-municipality",
   "metadata": {},
   "source": [
    "# Compute Gradient-based Causal Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "headed-browse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_causal_mask_with_precomputed(data_loader, all_importances, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        \n",
    "        orig_sample = tokenizer.decode(tokens)\n",
    "        causal_mask = mask_causal_words(tokens.cpu().numpy(), importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            # print(orig_sample[1], cf_sample[1])\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "                    #noncausal_mask_indices = np.random.choice(noncausal_indices, 1)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens)\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens)\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'Negative'\n",
    "            elif labels[0] == 1: label = 'Positive'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-indiana",
   "metadata": {},
   "source": [
    "# Compute Propensity-based Causal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "induced-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _TVD(orig_logits, cf_logits):\n",
    "    return 0.5 * torch.cdist(orig_logits.unsqueeze(0), cf_logits.unsqueeze(0), p=1).squeeze().item()\n",
    "\"\"\"For Debugging\"\"\"\n",
    "\"\"\"\n",
    "flipped_TVD = []\n",
    "unflipped_TVD = []\n",
    "def mask_softed_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        if orig_prediction != masked_prediction:\n",
    "            #DEBUGGING\n",
    "            flipped_TVD.append(_TVD(torch.softmax(orig_outputs[0], dim=-1), torch.softmax(masked_outputs[0], dim=-1)))\n",
    "            #DEBUGGING END\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "        else:\n",
    "            #DEBUGGING\n",
    "            unflipped_TVD.append(_TVD(torch.softmax(orig_outputs[0], dim=-1), torch.softmax(masked_outputs[0], dim=-1)))\n",
    "            #DEBUGGING END\n",
    "\n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag \n",
    "\"\"\"\n",
    "def mask_uniform_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    THRESHOLD = 0.1\n",
    "    uniform_dist = torch.ones(1, 2) / 2.0\n",
    "    uniform_dist = uniform_dist.to(\"cuda\")\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    best_tvd = 0.\n",
    "    best_idx = -1\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        orig_tvd = _TVD(torch.softmax(orig_outputs[0], dim=-1), uniform_dist)\n",
    "        masked_tvd = _TVD(torch.softmax(masked_outputs[0], dim=-1), uniform_dist)\n",
    "        \n",
    "        # Use maximum value\n",
    "        if orig_tvd > masked_tvd and abs(orig_tvd - masked_tvd) > best_tvd:\n",
    "            causal_mask[best_idx] = 0\n",
    "            causal_mask[topk_idx] = 1\n",
    "            best_tvd = abs(orig_tvd - masked_tvd)\n",
    "            best_idx = topk_idx\n",
    "        else:\n",
    "            continue\n",
    "        \"\"\"\n",
    "        # Use Gradient Order\n",
    "        if orig_tvd > masked_tvd:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        \"\"\"    \n",
    "        \n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag, best_tvd\n",
    "\n",
    "def mask_softed_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    best_tvd = 0.\n",
    "    best_idx = -1\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        tvd_value = _TVD(torch.softmax(orig_outputs[0], dim=-1), torch.softmax(masked_outputs[0], dim=-1))\n",
    "        \n",
    "        # Use Maximum  Value\n",
    "        if tvd_value > best_tvd:\n",
    "            causal_mask[best_idx] = 0\n",
    "            causal_mask[topk_idx] = 1\n",
    "            best_tvd = tvd_value\n",
    "            best_idx = topk_idx\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        \"\"\"\n",
    "        # Use Gradeint Order\n",
    "        if orig_tvd > MIN_FLIPPED:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        \"\"\"\n",
    "        \n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag, best_tvd\n",
    "\n",
    "def mask_propensity_causal_words(tokens, batch, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1]\n",
    "    err_flag = False\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    orig_outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    _, orig_prediction = torch.max(orig_outputs[0], dim=1)\n",
    "    for i, topk_idx in enumerate(topk_indices):\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[0][topk_idx + 1] = tokenizer.mask_token_id\n",
    "        masked_outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        _, masked_prediction = torch.max(masked_outputs[0], dim=1)\n",
    "        if orig_prediction != masked_prediction:\n",
    "            causal_mask[topk_idx] = 1\n",
    "            break\n",
    "    if 1 not in causal_mask:\n",
    "        causal_mask[topk_indices[0]] = 1\n",
    "        err_flag = True\n",
    "    return causal_mask, err_flag, 0\n",
    "\n",
    "def build_propensity_causal_mask_with_precomputed(data_loader, all_importances, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    error_cnt = 0\n",
    "    no_flip_cnt = 0\n",
    "    no_flip_idx = []\n",
    "    for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        \n",
    "        orig_sample = tokenizer.decode(tokens)\n",
    "        causal_mask, err_flag, maximum_score = mask_propensity_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        #causal_mask, err_flag, maximum_score = mask_softed_propensity_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        #causal_mask, err_flag, maximum_score = mask_uniform_propensity_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        no_flip_idx.append(err_flag)\n",
    "        if err_flag:\n",
    "            no_flip_cnt += 1\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            # print(orig_sample[1], cf_sample[1])\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                try:\n",
    "                    noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "                    #noncausal_mask_indices = np.random.choice(noncausal_indices, 1)\n",
    "                except:\n",
    "                    noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "                    error_cnt += 1\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens)\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens)\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'Negative'\n",
    "            elif labels[0] == 1: label = 'Positive'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample, err_flag, maximum_score))\n",
    "    print(f\"Error Cnt: {error_cnt}\")    \n",
    "    print(f\"No Flip Cnt: {no_flip_cnt}\")    \n",
    "    return triplets, no_flip_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-characteristic",
   "metadata": {},
   "source": [
    "# Compute OR Load Gradient Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "expired-lecture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PICKLE_PATH):\n",
    "    os.makedirs(PICKLE_PATH)\n",
    "\n",
    "#all_importance = compute_importances(train_loader, get_gradient_norms)\n",
    "#with open(\"dataset/SST-2/cf_augmented_examples/gradient_importance.pickle\", 'wb') as f:\n",
    "#    pickle.dump(all_importance, f)\n",
    "\n",
    "if os.path.exists(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\")):\n",
    "    with open(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\"), 'rb') as f:\n",
    "        all_importance = pickle.load(f)\n",
    "else:\n",
    "    all_importance = compute_importances(train_loader, get_gradient_norms)\n",
    "    with open(os.path.join(PICKLE_PATH, \"gradient_importance.pickle\"), 'wb') as f:\n",
    "        pickle.dump(all_importance, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-beatles",
   "metadata": {},
   "source": [
    "# Get Average Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cognitive-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(PICKLE_PATH, \"gradient_averaged_importance.pickle\")):\n",
    "    with open(os.path.join(PICKLE_PATH, \"gradient_averaged_importance.pickle\"), 'rb') as f:\n",
    "        averaged_all_importance = pickle.load(f)\n",
    "else:\n",
    "    raise ValueError(\"Please run averaged importance on original code.\")\n",
    "\n",
    "averaged_all_importance = averaged_all_importance[SPLIT_SAMPLES:SPLIT_SAMPLES*2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-chase",
   "metadata": {},
   "source": [
    "# Generate Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bb1c8e718c41aebbea21f18d25d4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampling_ratio = 1\n",
    "augment_ratio = 1\n",
    "triplets_train, no_flip_idx_train = build_propensity_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_flip_sampling{}_augmenting{}_train_cuda1.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "    pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-hygiene",
   "metadata": {},
   "source": [
    "# Cut with Empirically  Setted Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fatty-decline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6862\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.0001\n",
    "threshold_cnt = 0\n",
    "threshold_triplets_train = []\n",
    "for t in triplets_train:\n",
    "    t = list(t)\n",
    "    if t[-1] < THRESHOLD:\n",
    "        t[-2] = True\n",
    "        threshold_cnt += 1\n",
    "    t = tuple(t)\n",
    "    threshold_triplets_train.append(t)\n",
    "print(threshold_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "governmental-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_TVD_uniform_thres_00001_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "    pickle.dump(threshold_triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "black-chicago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844845040ff94300a1e99ec5efaf2d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5cce40e1d420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msampling_ratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maugment_ratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtriplets_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_flip_idx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_propensity_causal_mask_with_precomputed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maveraged_all_importance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPICKLE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"triplets_automated_averaged_gradient_propensity_sampling{}_augmenting{}_train.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-43b41559f1ab>\u001b[0m in \u001b[0;36mbuild_propensity_causal_mask_with_precomputed\u001b[0;34m(data_loader, all_importances, sampling_ratio, augment_ratio)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0morig_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_propensity_causal_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mno_flip_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merr_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-43b41559f1ab>\u001b[0m in \u001b[0;36mmask_propensity_causal_words\u001b[0;34m(tokens, batch, importances, topk)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmasked_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmasked_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopk_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmasked_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morig_prediction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmasked_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m         )\n\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m         )\n\u001b[1;32m    970\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    564\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 )\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         )\n\u001b[1;32m    462\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         )\n\u001b[1;32m    395\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###For Generating Optionally\n",
    "\n",
    "#for sampling_ratio in [1, 2, 3, 4, 5]:\n",
    "for sampling_ratio in [1]:\n",
    "    for augment_ratio in [1]:\n",
    "        triplets_train, no_flip_idx_train = build_propensity_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)\n",
    "        with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio)), \"wb\") as fp:\n",
    "            pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mathematical-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FLIPPED = 0.007849693298339844 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "steady-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015699736773967743\n",
      "0.5579570531845093\n"
     ]
    }
   ],
   "source": [
    "c = sorted(unflipped_TVD_logit)\n",
    "print(min(flipped_TVD_logit))\n",
    "print(c[int(len(c) * 0.90)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-prescription",
   "metadata": {},
   "source": [
    "# Qualitative Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHELL FOR QUALITATIVE: WILL BE REMOVED\\\n",
    "sampling_ratio = 1\n",
    "augment_ratio = 1\n",
    "triplets_train, no_flip_idx_train = build_propensity_causal_mask_with_precomputed(train_loader, averaged_all_importance, sampling_ratio=None, augment_ratio=augment_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dimensional-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_triplets = [tt for tt in triplets_train if tt[2].count(\"[MASK]\") > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "caring-format",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Positive',\n",
       " 'defuses this provocative theme by submerging it in a hoary love triangle.',\n",
       " 'def [MASK] this [MASK] theme by submerging it in a hoary love triangle.',\n",
       " '[MASK]uses [MASK] provocative [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]',\n",
       " False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qual_triplets[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "changing-south",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590\n"
     ]
    }
   ],
   "source": [
    "for i, tt in enumerate(triplets_train):\n",
    "    if tt[1] == 'commercialism all in the same movie... without neglecting character development for even one minute':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "alien-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_propensity_sampling1_augmenting1_train.pickle\".format(sampling_ratio, augment_ratio)), \"rb\") as fp:\n",
    "            data_b = pickle.load(fp)\n",
    "        \n",
    "with open(os.path.join(PICKLE_PATH, \"triplets_automated_averaged_gradient_sampling1_augmenting1_train.pickle\".format(sampling_ratio, augment_ratio)), \"rb\") as fp:\n",
    "            data_a = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "frequent-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "diff_idxes = []\n",
    "for i, (a, b) in enumerate(zip(data_a, data_b)):\n",
    "    if a[2] != b[2]:\n",
    "        diff_idxes.append(i)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_i = 3\n",
    "print(\"[ORIG]:    \" + data_a[diff_idxes[tmp_i]][1])\n",
    "print()\n",
    "print(\"[GRAD]:    \" + data_a[diff_idxes[tmp_i]][2])\n",
    "print()\n",
    "print(\"[PROP]:    \" + data_b[diff_idxes[tmp_i]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "western-figure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['commercial', '##ism', 'all', 'in', 'the', 'same', 'movie', '.', '.', '.', 'without', 'neglect', '##ing', 'character', 'development', 'for', 'even', 'one', 'minute']\n",
      "tensor([    0.0017,     0.0001,     0.0004,     0.0002,     0.0013,     0.0038,\n",
      "            0.0000,     0.0015,     0.0015,     0.0015,     0.0001,     0.0007,\n",
      "            0.0005,     0.0002,     0.0009,     0.0019,     0.0001,     0.0002,\n",
      "            0.0175])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=4,sci_mode=False)\n",
    "print(tokenizer.tokenize('commercialism all in the same movie... without neglecting character development for even one minute'))\n",
    "print(averaged_all_importance[1590])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "prescribed-cycling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Positive',\n",
       " 'commercialism all in the same movie... without neglecting character development for even one minute',\n",
       " 'commercialism all in the same movie... without neglecting character development for even one [MASK]',\n",
       " 'commercialism [MASK] in the same movie... without neglecting character development for even one minute')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_a[1590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "quantitative-scheduling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6942142857142857"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9719 / - (53349 - 67349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
