{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "legitimate-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expired-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/aclImdb\"\n",
    "REFORMED_DATASET_PATH = \"dataset/reform_aclImdb\"\n",
    "OUTPUT_PATH = \"checkpoints/not_augmented_output_scheduling_warmup\"\n",
    "REPS_PATH = \"reps\"\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "TRAIN_SPLIT = \"train\"\n",
    "TEST_SPLIT = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marine-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "changing-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"train.json\")) as f:\n",
    "#     train = json.load(f) \n",
    "# with open(os.path.join(REFORMED_DATASET_PATH, \"valid.json\")) as f:\n",
    "#     val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tamil-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = [d['text'] for d in train]\n",
    "# train_labels = [d['label'] for d in train]\n",
    "# val_texts = [d['text'] for d in val]\n",
    "# val_labels = [d['label'] for d in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civic-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optical-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class\n",
    "# train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "# val_dataset = IMDbDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "radical-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "authorized-equality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, 'epoch_2'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-carol",
   "metadata": {},
   "source": [
    "## Gradient-based Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "flush-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "paired = pickle.load(open(\"./dataset/cf_augmented_examples/triplets_sampling4_augmenting1_train.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "suburban-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [d[1] for d in paired]\n",
    "train_labels = [[1., 0.] if d[0] == 'Negative' else [0., 1.] for d in paired]\n",
    "\n",
    "# Encode dataset\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "# make dataset class\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "loving-bouquet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute gradient at BERT's position_embeddings (discard [cls] and [sep]/[pad])\n",
    "# Only works for batch_size = 1    \n",
    "def get_gradient_norms(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # For CrossEntropy Loss\n",
    "    _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    # print(loss)\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    importances = torch.tensor([]).to(device)\n",
    "    for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "        if token_index == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "        importance = torch.norm(model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad[pos_index], 2).float().detach()\n",
    "        importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    # importances_list.append(importances)\n",
    "    model._modules['bert']._modules['embeddings']._modules['position_embeddings'].weight.grad = None\n",
    "\n",
    "    # return importances_list\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "answering-mediterranean",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935143f9043c4a23ae356cf5003e55d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1700), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fabb60e298f4509865769b6f2b89537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1700), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69e85b1b1874af88a82caa20f4ec910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1700), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "def visualize(words, masks):\n",
    "    fig, ax = plt.subplots(figsize=(len(words), 1))\n",
    "    plt.rc('xtick', labelsize=16)\n",
    "    heatmap = sn.heatmap([masks], xticklabels=words, yticklabels=False, square=True, \\\n",
    "                         linewidths=0.1, cmap='coolwarm', center=0.5, vmin=0, vmax=1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "def mask_causal_words(tokens, importances, topk=1):\n",
    "    causal_mask = [0 for _ in range(len(tokens))]\n",
    "    topk_indices = np.argsort(importances)[::-1][:topk]\n",
    "    for topk_idx in topk_indices:\n",
    "#         print(topk_idx)\n",
    "#         print(tokens[topk_idx])\n",
    "        causal_mask[topk_idx] = 1\n",
    "    \n",
    "    return causal_mask\n",
    "\n",
    "def build_causal_mask(data_loader, sampling_ratio, augment_ratio):\n",
    "    triplets = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        importances = get_gradient_norms(batch)\n",
    "        \n",
    "        tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]])\n",
    "        assert tokens.size() == importances.size()\n",
    "        \n",
    "        orig_sample = tokenizer.decode(tokens)\n",
    "        causal_mask = mask_causal_words(tokens.cpu().numpy(), importances.cpu().numpy(), topk=sampling_ratio)\n",
    "        # visualize(tokens, causal_mask)\n",
    "        # print(causal_mask)\n",
    "        \n",
    "        if 1 not in causal_mask:\n",
    "            # print(orig_sample[1], cf_sample[1])\n",
    "            continue\n",
    "        \n",
    "        for _ in range(augment_ratio):\n",
    "            # 모든 causal 단어를 mask, 모든 non-causal 단어를 mask\n",
    "            if sampling_ratio is None:\n",
    "                causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "            # sampling_ratio 갯수 (int) 만큼의 단어를 mask\n",
    "            elif type(sampling_ratio) == int:\n",
    "                causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "                noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "                # print(causal_indices)\n",
    "\n",
    "                causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)                    \n",
    "                noncausal_mask_indices = np.random.choice(noncausal_indices, sampling_ratio)\n",
    "\n",
    "                causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "                noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "            \n",
    "            # sampling_ratio 비율 (%) 만큼의 단어를 mask\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            causal_masked_sample = tokenizer.decode(causal_masked_tokens)\n",
    "            noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens)\n",
    "            \n",
    "            _, labels = torch.max(batch['labels'], dim=1)\n",
    "            if labels[0] == 0: label = 'Negative'\n",
    "            elif labels[0] == 1: label = 'Positive'\n",
    "            triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample))\n",
    "        \n",
    "    return triplets\n",
    "\n",
    "import pickle\n",
    "\n",
    "# sampling_ratio = 2\n",
    "# augment_ratio = 1\n",
    "for sampling_ratio in [1, 2, 3]:\n",
    "    for augment_ratio in [1]:\n",
    "        triplets_train = build_causal_mask(train_loader, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)\n",
    "        with open(\"dataset/cf_augmented_examples/triplets_automated_gradient_sampling{}_augmenting{}_train.pickle\".format(sampling_ratio, augment_ratio), \"wb\") as fp:\n",
    "            pickle.dump(triplets_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dutch-midnight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Positive',\n",
       " 'p',\n",
       " \"zp is deeply related to that youth dream represented by the hippie movement. the college debate in the beginning of the movie states the cultural situation that gives birth to that movement. the explosion that daria imagines, represents the fall of all social structures and therefore the development of all that huge transformation that society is suffering through and finally mark's death anticipates the end that a sees for the movement itself. the film will be more easily understood if we go back to that time in life. during the 60'and 70 ', young people were the driving force for the profound explorations for change. one of the more significant changes intended was to bring sexuality out of the closet, and i think the scenes in the desert do not represent an orgy but the sexual relationship that men and women in absolute freedom would perform in the hipotetic situation where there would be nobody to hide from. i watched the scene where the couples would throw sand to each other and appreciated the [MASK] way in which a depicted the impossibility to continue hiding this basic human instinct. repression was the way to'control'social outbursts at that time and that is the method, police applies to stop the students. this society suffers from hipocresy, and that comes clear when the students gain access to weapons skipping all fake controls. the dialogue between the policeman with the college professor, who's detained for no reason shows part of society interested for this youth feeling and part completely uninterested. presenting flying as the more accurate symbol for freedom, the stealing of the plane represents mark's inner wish for it [MASK], his ( going back or coming back or returning ( segun ) ) shows the [MASK] to come free from these bonds and as i've said, a depicts the death of the dream by these difficulties winning the game. in my point of view a film to remember.\",\n",
       " \"zp is deeply related to that youth dream represented by the hippie movement. the college debate in the beginning of the movie states the cultural situation that gives birth to that movement. the explosion that daria imagines, represents the fall of all social structures and therefore the development of all that huge transformation that society is suffering through and finally mark's death anticipates the end that a sees for the movement itself. the film will be more easily understood if we go back to that time in life. during [MASK] 60'and 70 ', young people were the driving force for the profound explorations for change. one of the more significant changes intended was to bring sexuality out of the closet, and i think the scenes in the desert do not represent an orgy but the sexual relationship that men and women in absolute freedom would perform in the hipotetic situation where there would be nobody to hide from. i watched the scene where the couples would throw sand to each other and appreciated the magnificent way in which a depicted the impossibility to continue hiding this basic human instinct. repression was the way to'control'social outbursts at that time and that is the method, police applies to stop the students. this society [MASK] from hipocresy, and [MASK] comes clear when the students gain access to weapons skipping all fake controls. the dialogue between the policeman with the college professor, who's detained for no reason shows part of society interested for this youth feeling and part completely uninterested. presenting flying as the more accurate symbol for freedom, the stealing of the plane represents mark's inner wish for it but, his ( going back or coming back or returning ( segun ) ) shows the difficulties to come free from these bonds and as i've said, a depicts the death of the dream by these difficulties winning the game. in my point of view a film to remember.\")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_train[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
